{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UELB0UvmV5jD",
        "outputId": "ddc025cf-6317-4afb-de5e-8a2361e009b7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import tensorflow\n",
        "import keras\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "from tensorflow.keras import backend as K\n",
        "# for BERT\n",
        "!pip install transformers\n",
        "import transformers\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 607,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCkWxXQgjno"
      },
      "source": [
        "Read the data in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVEDcnc6bI8K"
      },
      "source": [
        "df = pd.read_csv(\"gdrive/My Drive/twitter_data.csv\", encoding='latin-1', header= None) # Read the data in"
      ],
      "execution_count": 608,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Vz1UAJYAd4Al",
        "outputId": "c8487095-bfa9-48e1-b0e3-7e71c3b5cd97"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 609,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  ...                                                  5\n",
              "0  0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1  0  ...  is upset that he can't update his Facebook by ...\n",
              "2  0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3  0  ...    my whole body feels itchy and like its on fire \n",
              "4  0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 609
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgo3iLQVjd77"
      },
      "source": [
        "**target:** the polarity of the tweet (0 = negative, 4 = positive) \n",
        "\n",
        "**ids:** The id of the tweet ( 2087)\n",
        "\n",
        "**date:** the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "\n",
        "**flag:** The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "\n",
        "**user:** the user that tweeted (robotickilldozr)\n",
        "\n",
        "**text:** the text of the tweet (Lyx is cool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxWn5UHGgnQB"
      },
      "source": [
        "Pick out the relavant data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yhZYzGPfxe3"
      },
      "source": [
        "df = df.drop(columns= [1, 2, 3, 4])  # Drop all useless data\n",
        "df.columns = ['emotion', 'tweet'] # Rename the relavant columns"
      ],
      "execution_count": 610,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2HV8WrhgtYe"
      },
      "source": [
        "Too much data is given here so lets cut down on the amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA0ULAsDgrUq",
        "outputId": "8ee53eb3-8dd3-4017-cba8-49bf972c924e"
      },
      "source": [
        "len(df)  # there was 1.6 mill data here. Im using google collab here so it'll definitely crash the computer"
      ],
      "execution_count": 611,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 611
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch20SwARhJe8"
      },
      "source": [
        "I will decide upon having 60, 000 data for the train data and 20000 for test data. There are only 2 sentiments. To ensure its a fair process i'll take 30, 000 tweets from each category for train and 10,000 for test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdxHpzUahdTM"
      },
      "source": [
        "# create new df containing negative sentiment\n",
        "df_negative = df[df['emotion'] == 0]\n",
        "\n",
        "# create new df containing positive sentiment\n",
        "df_positive = df[df['emotion'] == 4]"
      ],
      "execution_count": 612,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaLVJzC90HRU"
      },
      "source": [
        "# This ensures we only get tweets that are less than 50 words long\n",
        "\n",
        "df_negative = df_negative[df_negative['tweet'].apply(lambda x: len(x.split(\" \")) <= 48)]\n",
        "\n",
        "df_positive = df_positive[df_positive['tweet'].apply(lambda x: len(x.split(\" \")) <= 48)]"
      ],
      "execution_count": 613,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geOKGbC8lve5"
      },
      "source": [
        "from sklearn.utils import shuffle  # to mix things up a bi t and introduce some randomness\n",
        "df_negative = shuffle(df_negative)\n",
        "\n",
        "df_positive = shuffle(df_positive)"
      ],
      "execution_count": 614,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAq0GWRuhdQ9"
      },
      "source": [
        "df_negative = df_negative[:40000]\n",
        "df_positive = df_positive[:40000]"
      ],
      "execution_count": 615,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4tcHNZWN0Hz"
      },
      "source": [
        "Shuffle one more time for good measure (shuffling introduces randomness)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SZR42cFNsZ4"
      },
      "source": [
        "df_negative = shuffle(df_negative)\n",
        "\n",
        "df_positive = shuffle(df_positive)"
      ],
      "execution_count": 616,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pt9njAOmnSY"
      },
      "source": [
        "Before we split them into test and train lets look at the data again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "E_QOuVUNmxn4",
        "outputId": "b3cdd502-fa9a-4b90-ec0c-339f42bc1bbd"
      },
      "source": [
        "df_negative = df_negative.reset_index(drop=True)\n",
        "\n",
        "df_positive = df_positive.reset_index(drop=True)\n",
        "\n",
        "df_negative"
      ],
      "execution_count": 617,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>just got back from the show, it was hilrious !...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Its June 6th 2009, my birthday. Its 2:14 A.M. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>leaving buena park  but ill be back in a few w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>doesnt want keith to die  ...might skip that e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>got burnt today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>0</td>\n",
              "      <td>I'm so tired and suddenly I think covering tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>0</td>\n",
              "      <td>Another friend knocked up.   Pretty soon every...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>0</td>\n",
              "      <td>stuying commercial law ALL DAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>0</td>\n",
              "      <td>@marano2288 hahaha. Because Toucan Sam has a b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>0</td>\n",
              "      <td>@meximikefsu  they have them at whole foods or...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                              tweet\n",
              "0            0  just got back from the show, it was hilrious !...\n",
              "1            0  Its June 6th 2009, my birthday. Its 2:14 A.M. ...\n",
              "2            0  leaving buena park  but ill be back in a few w...\n",
              "3            0  doesnt want keith to die  ...might skip that e...\n",
              "4            0                                   got burnt today \n",
              "...        ...                                                ...\n",
              "39995        0  I'm so tired and suddenly I think covering tha...\n",
              "39996        0  Another friend knocked up.   Pretty soon every...\n",
              "39997        0                    stuying commercial law ALL DAY \n",
              "39998        0  @marano2288 hahaha. Because Toucan Sam has a b...\n",
              "39999        0  @meximikefsu  they have them at whole foods or...\n",
              "\n",
              "[40000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 617
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9wnScPom3DA"
      },
      "source": [
        "You can see that some tweets has \n",
        "\n",
        "**@username_blah1**\n",
        "\n",
        "**hastag#Glazers_out**\n",
        "\n",
        "**links like (http://, https://, www. ect..)**\n",
        "\n",
        "These are unimportant and shoudnt bear importance in negative/positive (Or at least i'll take it as such)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2iDnRBLhcQ6"
      },
      "source": [
        "**More data cleaning is thus required**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFbDj7k4e4sY",
        "outputId": "81c18e49-2a30-4986-bc26-7aad0e15c85e"
      },
      "source": [
        "type(df_negative['tweet'][0]) # the data in tweet is of type string"
      ],
      "execution_count": 618,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 618
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylawH_pvghqo",
        "outputId": "4dbbc0b5-68da-45a1-d0b7-404b31c5427a"
      },
      "source": [
        "# these regexesr enable us to filter out unwanted data\n",
        "regex1 = re.compile('@') # format to remove @username_chungus from a string\n",
        "regex2 = re.compile('#') # format to remove #Ole'sAtTheWheel from a string\n",
        "regex3 = re.compile('http') # format to remove http/https://blahblah.com (Dont blame me if you click this link i dunno where it goes) from a string\n",
        "regex4 = re.compile('www') # format to remove www.blahblah.com from a string\n",
        "regex5 = re.compile('\\d') # format to remove numbers from a string\n",
        "regex6 = re.compile(r'[^\\w\\s]') # format to remove punctuations from a list\n",
        "\n",
        "# I want to know what the max length of a tweet is\n",
        "max_negative_tweet_length = -10\n",
        "max_positive_tweet_length = -10\n",
        "\n",
        "for i in range(len(df_negative)):\n",
        "  process_negative = df_negative['tweet'][i]\n",
        "  process_negative = process_negative.split(\" \")\n",
        "  \n",
        "  process_negative = [s for s in process_negative if not regex1.match(s)]\n",
        "  process_negative = [s for s in process_negative if not regex2.match(s)]\n",
        "  process_negative = [s for s in process_negative if not regex3.match(s)]\n",
        "  process_negative = [s for s in process_negative if not regex4.match(s)]\n",
        "  process_negative = [s for s in process_negative if not regex5.match(s)]\n",
        "  process_negative = [s for s in process_negative if not regex6.match(s)]\n",
        "\n",
        "  if (len(process_negative) > max_negative_tweet_length):\n",
        "    max_negative_tweet_length = len(process_negative)\n",
        "\n",
        "  df_negative['tweet'][i] =  \" \".join(process_negative)\n",
        "  df_negative['tweet'][i] = re.sub(r'[^\\w\\s]', '', df_negative['tweet'][i]) # format to remove punctuations from a string\n",
        "\n",
        " # Now for the positive portion\n",
        "  process_positive = df_positive['tweet'][i]\n",
        "  process_positive = process_positive.split(\" \")\n",
        "  \n",
        "  process_positive = [s for s in process_positive if not regex1.match(s)]\n",
        "  process_positive = [s for s in process_positive if not regex2.match(s)]\n",
        "  process_positive = [s for s in process_positive if not regex3.match(s)]\n",
        "  process_positive = [s for s in process_positive if not regex4.match(s)]\n",
        "  process_positive = [s for s in process_positive if not regex5.match(s)]\n",
        "  process_positive = [s for s in process_positive if not regex6.match(s)]\n",
        "\n",
        "  if (len(process_positive) > max_positive_tweet_length):\n",
        "    max_positive_tweet_length = len(process_positive)\n",
        "    \n",
        "  df_positive['tweet'][i] =  \" \".join(process_positive)\n",
        "  df_positive['tweet'][i] = re.sub(r'[^\\w\\s]', '', df_positive['tweet'][i]) # format to remove punctuations from a string\n"
      ],
      "execution_count": 619,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Ga8kEcx0FL",
        "outputId": "aa5cc576-28af-4f37-cefe-ce98ad2eb295"
      },
      "source": [
        "print(\"Maximum length of a negative tweet is: \", max_negative_tweet_length)\n",
        "print(\"Maximum length of a positive tweet is: \", max_positive_tweet_length)"
      ],
      "execution_count": 620,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum length of a negative tweet is:  41\n",
            "Maximum length of a positive tweet is:  34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq4c4BvQyKxk"
      },
      "source": [
        "The disparity is quite big so this does show that we should have indicated a maximum number of words in a tweet from the start. \n",
        "\n",
        "By the time you are reading this i would have already done so, so no worries :)\n",
        "\n",
        "The concept behind this is to ensure that the vector space wouldnt be too sparse. Else we would require a lot (A lot) of padding, and this might cause the model to perform poorly. (Also i'likely run out of resources too).\n",
        "\n",
        "A solution to this is to just pick a number to be the max length. For our case, 70 sounds good enough. However, this also means our model would be unable to classify tweets that are more than 70 words long. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fjX5M3CwFYh"
      },
      "source": [
        "**Data cleaning should be done. We can now begin tokenizing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "QzyvD_vi6TUn",
        "outputId": "19072fbb-73a0-444d-b6f1-bed0a70d8808"
      },
      "source": [
        "df_positive"
      ],
      "execution_count": 621,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>Carrie Fisher Im just on a big biography kick ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Off to the grandparents house for dinner  Peac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>just finished eating a slice of strawberry pie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Yep its mine  Went to the screening in Orlando...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Enjoying d view sg flyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>4</td>\n",
              "      <td>Glad you like them  x</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>4</td>\n",
              "      <td>i truly felt blessed and feel free of all the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>4</td>\n",
              "      <td>Woahhhmarie just got jimmyjackeddd hahahahaa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>4</td>\n",
              "      <td>at least if you forget it theres always youtube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>4</td>\n",
              "      <td>WTF is wrong with my icon oh well work countdo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion                                              tweet\n",
              "0            4  Carrie Fisher Im just on a big biography kick ...\n",
              "1            4  Off to the grandparents house for dinner  Peac...\n",
              "2            4  just finished eating a slice of strawberry pie...\n",
              "3            4  Yep its mine  Went to the screening in Orlando...\n",
              "4            4                          Enjoying d view sg flyer \n",
              "...        ...                                                ...\n",
              "39995        4                              Glad you like them  x\n",
              "39996        4  i truly felt blessed and feel free of all the ...\n",
              "39997        4      Woahhhmarie just got jimmyjackeddd hahahahaa \n",
              "39998        4   at least if you forget it theres always youtube \n",
              "39999        4  WTF is wrong with my icon oh well work countdo...\n",
              "\n",
              "[40000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 621
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-n-lDUb8ZCJ"
      },
      "source": [
        "Looking good. The text has been cleaned. More or less. We can now proceed to perform **tokenizing** via the BERT provided one. It's some good stuff. We'll than need to do the **masking** and **padding** as required by BERT.\n",
        "\n",
        "The BERT model accepts inputs using as a matrix of these three inputs (in bold)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEpwh9SgwUwk"
      },
      "source": [
        "## distil-bert tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 622,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2BmPLv0ZIs"
      },
      "source": [
        "df_negative['tweet_token'] = 'hi'\n",
        "df_positive['tweet_token'] = 'hi'"
      ],
      "execution_count": 623,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ETgu1kN1jtT"
      },
      "source": [
        "**Generate tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtnzj_Aq1hwR",
        "outputId": "0826c224-9168-4ed3-ce22-7d6a88723140"
      },
      "source": [
        "mn = -10\n",
        "mp = -10\n",
        "for i in range(len(df_negative)):\n",
        "  if ( len(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_negative['tweet'][i].lower().strip()))  ) > mn):\n",
        "    mn = len(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_negative['tweet'][i].lower().strip()))  )\n",
        "\n",
        "  df_negative['tweet_token'][i] = \" \".join(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_negative['tweet'][i].lower().strip()))  )\n",
        "\n",
        "  if ( len(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_positive['tweet'][i].lower().strip()))  ) > mp):\n",
        "    mp = len(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_positive['tweet'][i].lower().strip()))  )\n",
        "\n",
        "  df_positive['tweet_token'][i] = \" \".join(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_positive['tweet'][i].lower().strip()))  )"
      ],
      "execution_count": 624,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWf0O7qO2rOv",
        "outputId": "7f908c14-96d7-4d8d-dc5f-007d8c8e8f78"
      },
      "source": [
        "print(mn)\n",
        "\n",
        "print(mp)"
      ],
      "execution_count": 625,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n",
            "129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIZJhXeG28H2"
      },
      "source": [
        "maxlen = 70"
      ],
      "execution_count": 626,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "coXUZS0b36lC",
        "outputId": "936632f7-9fc1-47b4-9cb8-fedd7b664d4c"
      },
      "source": [
        "df_negative"
      ],
      "execution_count": 627,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>just got back from the show it was hilrious i ...</td>\n",
              "      <td>just got back from the show it was hi ##lr ##i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Its June my birthday Its AM and I cant even sl...</td>\n",
              "      <td>its june my birthday its am and i can ##t even...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>leaving buena park  but ill be back in a few w...</td>\n",
              "      <td>leaving buena park but ill be back in a few weeks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>doesnt want keith to die  skip that episode ou...</td>\n",
              "      <td>doesn ##t want keith to die skip that episode ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>got burnt today</td>\n",
              "      <td>got burnt today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>0</td>\n",
              "      <td>Im so tired and suddenly I think covering that...</td>\n",
              "      <td>im so tired and suddenly i think covering that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>0</td>\n",
              "      <td>Another friend knocked up   Pretty soon everyo...</td>\n",
              "      <td>another friend knocked up pretty soon everyone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>0</td>\n",
              "      <td>stuying commercial law ALL DAY</td>\n",
              "      <td>stu ##ying commercial law all day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>0</td>\n",
              "      <td>hahaha Because Toucan Sam has a beak  Im so la...</td>\n",
              "      <td>ha ##ha ##ha because to ##uca ##n sam has a be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>0</td>\n",
              "      <td>they have them at whole foods or any vitamin ...</td>\n",
              "      <td>they have them at whole foods or any vitamin s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion  ...                                        tweet_token\n",
              "0            0  ...  just got back from the show it was hi ##lr ##i...\n",
              "1            0  ...  its june my birthday its am and i can ##t even...\n",
              "2            0  ...  leaving buena park but ill be back in a few weeks\n",
              "3            0  ...  doesn ##t want keith to die skip that episode ...\n",
              "4            0  ...                                    got burnt today\n",
              "...        ...  ...                                                ...\n",
              "39995        0  ...  im so tired and suddenly i think covering that...\n",
              "39996        0  ...  another friend knocked up pretty soon everyone...\n",
              "39997        0  ...                  stu ##ying commercial law all day\n",
              "39998        0  ...  ha ##ha ##ha because to ##uca ##n sam has a be...\n",
              "39999        0  ...  they have them at whole foods or any vitamin s...\n",
              "\n",
              "[40000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 627
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaKG2Jjc3O54"
      },
      "source": [
        "## add special tokens\n",
        "maxqnans = np.int(maxlen-20)  # The tokenized from would end up being too long as seen by the values of mn and np, thus we would just take the first #maxqns (70 - 20 = 50) number of tokens \n",
        "df_negative_tokenized_list = [\"[CLS] \"+  \" \".join(txt.split(\" \")[:maxqnans]) + \" [SEP]\" for txt in df_negative[\"tweet_token\"]]\n",
        "\n",
        "df_positive_tokenized_list = [\"[CLS] \"+  \" \".join((txt.split(\" \"))[:maxqnans]) + \" [SEP]\" for txt in df_positive[\"tweet_token\"]]\n"
      ],
      "execution_count": 628,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BDIh-hTcIbXm",
        "outputId": "5fc69a5f-a320-4ca3-cd55-072a1ef3aa39"
      },
      "source": [
        "df_negative_tokenized_list[39997]"
      ],
      "execution_count": 629,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] stu ##ying commercial law all day [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 629
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ks8SKCv42aH"
      },
      "source": [
        "**Generate mask**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtRaFi5X40q8"
      },
      "source": [
        "## generate masks for negative\n",
        "masks_negative = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in df_negative_tokenized_list]\n",
        "\n",
        "## generate masks for positive\n",
        "masks_positive = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in df_positive_tokenized_list]"
      ],
      "execution_count": 630,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOqdSG-66SNI"
      },
      "source": [
        "**Perform padding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg3-uJBQ6UHe"
      },
      "source": [
        "## padding for negative\n",
        "txt2seq_negative = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in df_negative_tokenized_list]\n",
        "\n",
        "## padding for positive\n",
        "txt2seq_positive = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in df_positive_tokenized_list]"
      ],
      "execution_count": 631,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVjgqo8G66kn"
      },
      "source": [
        "**Now we can generate the ids (We give the words unique ids (Similar idea to one hot encoding, but using a different idea)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1zMRqFq61mH"
      },
      "source": [
        "## generate idx for negative\n",
        "idx_negative = []\n",
        "\n",
        "for seq in txt2seq_negative:\n",
        "  the_id = tokenizer.convert_tokens_to_ids(seq.split(\" \")) \n",
        "  idx_negative.append(the_id)\n",
        "\n",
        "## generate idx for positive\n",
        "idx_positive = []\n",
        "\n",
        "for seq in txt2seq_positive:\n",
        "  the_id = tokenizer.convert_tokens_to_ids(seq.split(\" \")) \n",
        "  idx_positive.append(the_id)"
      ],
      "execution_count": 632,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOK_qj1O7Z_-"
      },
      "source": [
        "**Generate segments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RJNOL807M0u"
      },
      "source": [
        "## generate segments for negative\n",
        "segments_negative = [] \n",
        "for seq in txt2seq_negative:\n",
        "  temp, i = [], 0\n",
        "  for token in seq.split(\" \"):\n",
        "    if token != \"[SEP]\":\n",
        "      temp.append(i)\n",
        "    else:\n",
        "      i = 1\n",
        "      temp.append(i)\n",
        "  segments_negative.append(temp)\n",
        "\n",
        "## generate segments for positive\n",
        "segments_positive = [] \n",
        "for seq in txt2seq_positive:\n",
        "  temp, i = [], 0\n",
        "  for token in seq.split(\" \"):\n",
        "    if token != \"[SEP]\":\n",
        "      temp.append(i)\n",
        "    else:\n",
        "      i = 1\n",
        "      temp.append(i)\n",
        "  segments_positive.append(temp)"
      ],
      "execution_count": 633,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrlOdDygHS_6"
      },
      "source": [
        "**Print some examples out as a sanity check. Lets use the magic numbers ;)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVk3b-BaFhME",
        "outputId": "da920a37-b6d5-4172-ac49-9392a91a7c2f"
      },
      "source": [
        "print(df_negative['tweet'][420])\n",
        "print(df_negative['tweet_token'][420])\n",
        "print(df_negative_tokenized_list[420])\n",
        "print(masks_negative[420])\n",
        "print(txt2seq_negative[420])\n",
        "print(idx_negative[420])\n",
        "print(segments_negative[420])\n"
      ],
      "execution_count": 634,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I still am young and innocent  when Its really hot I almost drowned though \n",
            "i still am young and innocent when its really hot i almost drowned though\n",
            "[CLS] i still am young and innocent when its really hot i almost drowned though [SEP]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[CLS] i still am young and innocent when its really hot i almost drowned though [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[101, 1045, 2145, 2572, 2402, 1998, 7036, 2043, 2049, 2428, 2980, 1045, 2471, 12805, 2295, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA47haagHOJb",
        "outputId": "58b98c77-88a1-4384-c569-a7b747c5856f"
      },
      "source": [
        "print(df_negative['tweet'][69])\n",
        "print(df_negative['tweet_token'][69])\n",
        "print(df_negative_tokenized_list[69])\n",
        "print(masks_negative[69])\n",
        "print(txt2seq_negative[69])\n",
        "print(idx_negative[69])\n",
        "print(segments_negative[69])"
      ],
      "execution_count": 635,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "School sports were the best Pity everytime we had netball practice we forgot our PE glothes at home \n",
            "school sports were the best pity every ##time we had netball practice we forgot our pe g ##lot ##hes at home\n",
            "[CLS] school sports were the best pity every ##time we had netball practice we forgot our pe g ##lot ##hes at home [SEP]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[CLS] school sports were the best pity every ##time we had netball practice we forgot our pe g ##lot ##hes at home [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[101, 2082, 2998, 2020, 1996, 2190, 12063, 2296, 7292, 2057, 2018, 25034, 3218, 2057, 9471, 2256, 21877, 1043, 10994, 15689, 2012, 2188, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kdRRuEbPLFG"
      },
      "source": [
        "df_negative[\"token_id\"] = idx_negative\n",
        "df_negative[\"masks\"] = masks_negative\n",
        "df_negative[\"segments\"] = segments_negative\n",
        "\n",
        "df_positive[\"token_id\"] = idx_positive\n",
        "df_positive[\"masks\"] = masks_positive\n",
        "df_positive[\"segments\"] = segments_positive"
      ],
      "execution_count": 636,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCIZOQsiLWpM"
      },
      "source": [
        "# **We can now begin preparing the train and test data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfzF7DNKLm2O"
      },
      "source": [
        "We have emotion = 0 representing negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "6qzcx6NbLUn-",
        "outputId": "82b39a39-6dd4-4a18-b715-ad7928181799"
      },
      "source": [
        "df_negative"
      ],
      "execution_count": 637,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>token_id</th>\n",
              "      <th>masks</th>\n",
              "      <th>segments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>just got back from the show it was hilrious i ...</td>\n",
              "      <td>just got back from the show it was hi ##lr ##i...</td>\n",
              "      <td>[101, 2074, 2288, 2067, 2013, 1996, 2265, 2009...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Its June my birthday Its AM and I cant even sl...</td>\n",
              "      <td>its june my birthday its am and i can ##t even...</td>\n",
              "      <td>[101, 2049, 2238, 2026, 5798, 2049, 2572, 1998...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>leaving buena park  but ill be back in a few w...</td>\n",
              "      <td>leaving buena park but ill be back in a few weeks</td>\n",
              "      <td>[101, 2975, 27493, 2380, 2021, 5665, 2022, 206...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>doesnt want keith to die  skip that episode ou...</td>\n",
              "      <td>doesn ##t want keith to die skip that episode ...</td>\n",
              "      <td>[101, 2987, 2102, 2215, 6766, 2000, 3280, 1355...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>got burnt today</td>\n",
              "      <td>got burnt today</td>\n",
              "      <td>[101, 2288, 11060, 2651, 102, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>0</td>\n",
              "      <td>Im so tired and suddenly I think covering that...</td>\n",
              "      <td>im so tired and suddenly i think covering that...</td>\n",
              "      <td>[101, 10047, 2061, 5458, 1998, 3402, 1045, 222...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>0</td>\n",
              "      <td>Another friend knocked up   Pretty soon everyo...</td>\n",
              "      <td>another friend knocked up pretty soon everyone...</td>\n",
              "      <td>[101, 2178, 2767, 6573, 2039, 3492, 2574, 3071...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>0</td>\n",
              "      <td>stuying commercial law ALL DAY</td>\n",
              "      <td>stu ##ying commercial law all day</td>\n",
              "      <td>[101, 24646, 14147, 3293, 2375, 2035, 2154, 10...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>0</td>\n",
              "      <td>hahaha Because Toucan Sam has a beak  Im so la...</td>\n",
              "      <td>ha ##ha ##ha because to ##uca ##n sam has a be...</td>\n",
              "      <td>[101, 5292, 3270, 3270, 2138, 2000, 18100, 207...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>0</td>\n",
              "      <td>they have them at whole foods or any vitamin ...</td>\n",
              "      <td>they have them at whole foods or any vitamin s...</td>\n",
              "      <td>[101, 2027, 2031, 2068, 2012, 2878, 9440, 2030...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion  ...                                           segments\n",
              "0            0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1            0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2            0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...\n",
              "3            0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "4            0  ...  [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "...        ...  ...                                                ...\n",
              "39995        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39996        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39997        0  ...  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "39998        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39999        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[40000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 637
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHO_84lpLqDI"
      },
      "source": [
        "We have emotion = 4 representng positive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "qo_CuNgKLhwq",
        "outputId": "9ba39374-7145-4cd6-8530-3ba02f07b845"
      },
      "source": [
        "df_positive"
      ],
      "execution_count": 638,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>token_id</th>\n",
              "      <th>masks</th>\n",
              "      <th>segments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>Carrie Fisher Im just on a big biography kick ...</td>\n",
              "      <td>carrie fisher im just on a big biography kick ...</td>\n",
              "      <td>[101, 13223, 8731, 10047, 2074, 2006, 1037, 25...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Off to the grandparents house for dinner  Peac...</td>\n",
              "      <td>off to the grandparents house for dinner peace...</td>\n",
              "      <td>[101, 2125, 2000, 1996, 14472, 2160, 2005, 459...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>just finished eating a slice of strawberry pie...</td>\n",
              "      <td>just finished eating a slice of strawberry pie...</td>\n",
              "      <td>[101, 2074, 2736, 5983, 1037, 14704, 1997, 168...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Yep its mine  Went to the screening in Orlando...</td>\n",
              "      <td>yep its mine went to the screening in orlando ...</td>\n",
              "      <td>[101, 15624, 2049, 3067, 2253, 2000, 1996, 113...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Enjoying d view sg flyer</td>\n",
              "      <td>enjoying d view sg flyer</td>\n",
              "      <td>[101, 9107, 1040, 3193, 22214, 23821, 102, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>4</td>\n",
              "      <td>Glad you like them  x</td>\n",
              "      <td>glad you like them x</td>\n",
              "      <td>[101, 5580, 2017, 2066, 2068, 1060, 102, 0, 0,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>4</td>\n",
              "      <td>i truly felt blessed and feel free of all the ...</td>\n",
              "      <td>i truly felt blessed and feel free of all the ...</td>\n",
              "      <td>[101, 1045, 5621, 2371, 10190, 1998, 2514, 248...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>4</td>\n",
              "      <td>Woahhhmarie just got jimmyjackeddd hahahahaa</td>\n",
              "      <td>wo ##ah ##hh ##mar ##ie just got jimmy ##jack ...</td>\n",
              "      <td>[101, 24185, 4430, 23644, 7849, 2666, 2074, 22...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>4</td>\n",
              "      <td>at least if you forget it theres always youtube</td>\n",
              "      <td>at least if you forget it there ##s always you...</td>\n",
              "      <td>[101, 2012, 2560, 2065, 2017, 5293, 2009, 2045...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>4</td>\n",
              "      <td>WTF is wrong with my icon oh well work countdo...</td>\n",
              "      <td>w ##tf is wrong with my icon oh well work coun...</td>\n",
              "      <td>[101, 1059, 24475, 2003, 3308, 2007, 2026, 126...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion  ...                                           segments\n",
              "0            4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...\n",
              "1            4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2            4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3            4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4            4  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "...        ...  ...                                                ...\n",
              "39995        4  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "39996        4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39997        4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39998        4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...\n",
              "39999        4  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[40000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 638
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mHzDVzMt2O"
      },
      "source": [
        "**For readability sake, lets redefine having value 1 as having positive sentiment.**\n",
        "\n",
        "**Thus. emotion = 0 = negative sentiment,**\n",
        "\n",
        "**and emotion = 1 = positive sentiment.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-yqiavjMkqK"
      },
      "source": [
        "df_positive['emotion'] = 1"
      ],
      "execution_count": 639,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "d_ufc5zMMoLO",
        "outputId": "f9669476-66c1-4497-95de-5c220e3447f2"
      },
      "source": [
        "df_positive"
      ],
      "execution_count": 640,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>token_id</th>\n",
              "      <th>masks</th>\n",
              "      <th>segments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Carrie Fisher Im just on a big biography kick ...</td>\n",
              "      <td>carrie fisher im just on a big biography kick ...</td>\n",
              "      <td>[101, 13223, 8731, 10047, 2074, 2006, 1037, 25...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Off to the grandparents house for dinner  Peac...</td>\n",
              "      <td>off to the grandparents house for dinner peace...</td>\n",
              "      <td>[101, 2125, 2000, 1996, 14472, 2160, 2005, 459...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>just finished eating a slice of strawberry pie...</td>\n",
              "      <td>just finished eating a slice of strawberry pie...</td>\n",
              "      <td>[101, 2074, 2736, 5983, 1037, 14704, 1997, 168...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Yep its mine  Went to the screening in Orlando...</td>\n",
              "      <td>yep its mine went to the screening in orlando ...</td>\n",
              "      <td>[101, 15624, 2049, 3067, 2253, 2000, 1996, 113...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Enjoying d view sg flyer</td>\n",
              "      <td>enjoying d view sg flyer</td>\n",
              "      <td>[101, 9107, 1040, 3193, 22214, 23821, 102, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39995</th>\n",
              "      <td>1</td>\n",
              "      <td>Glad you like them  x</td>\n",
              "      <td>glad you like them x</td>\n",
              "      <td>[101, 5580, 2017, 2066, 2068, 1060, 102, 0, 0,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39996</th>\n",
              "      <td>1</td>\n",
              "      <td>i truly felt blessed and feel free of all the ...</td>\n",
              "      <td>i truly felt blessed and feel free of all the ...</td>\n",
              "      <td>[101, 1045, 5621, 2371, 10190, 1998, 2514, 248...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39997</th>\n",
              "      <td>1</td>\n",
              "      <td>Woahhhmarie just got jimmyjackeddd hahahahaa</td>\n",
              "      <td>wo ##ah ##hh ##mar ##ie just got jimmy ##jack ...</td>\n",
              "      <td>[101, 24185, 4430, 23644, 7849, 2666, 2074, 22...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39998</th>\n",
              "      <td>1</td>\n",
              "      <td>at least if you forget it theres always youtube</td>\n",
              "      <td>at least if you forget it there ##s always you...</td>\n",
              "      <td>[101, 2012, 2560, 2065, 2017, 5293, 2009, 2045...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39999</th>\n",
              "      <td>1</td>\n",
              "      <td>WTF is wrong with my icon oh well work countdo...</td>\n",
              "      <td>w ##tf is wrong with my icon oh well work coun...</td>\n",
              "      <td>[101, 1059, 24475, 2003, 3308, 2007, 2026, 126...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       emotion  ...                                           segments\n",
              "0            1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...\n",
              "1            1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2            1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3            1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4            1  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "...        ...  ...                                                ...\n",
              "39995        1  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "39996        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39997        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "39998        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...\n",
              "39999        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[40000 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 640
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3wA1JCpQop2"
      },
      "source": [
        "One more shuffle. I made sure to reset the index for completeness sake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq9lGJeKQi4l"
      },
      "source": [
        "df_negative = shuffle(df_negative)\n",
        "df_negative = df_negative.reset_index(drop=True)\n",
        "\n",
        "df_positive = shuffle(df_positive)\n",
        "df_positive = df_positive.reset_index(drop=True)"
      ],
      "execution_count": 641,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qYoutBaQi4m"
      },
      "source": [
        "Before we split them into test and train lets look at the data again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fm6TbYDQ1Jq"
      },
      "source": [
        "**We can finally break them into train and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqLSLl8SQi4m"
      },
      "source": [
        "negative_train = df_negative[:30000]\n",
        "negative_train = negative_train.reset_index(drop=True)\n",
        "\n",
        "positive_train = df_positive[:30000]\n",
        "positive_train = positive_train.reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "negative_test = df_negative[30000:]\n",
        "negative_test = negative_test.reset_index(drop=True)\n",
        "\n",
        "positive_test = df_positive[30000:]\n",
        "positive_test = positive_test.reset_index(drop=True)"
      ],
      "execution_count": 642,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojq2avzDRxGI"
      },
      "source": [
        "train_df = pd.concat([negative_train, positive_train])\n",
        "train_df = shuffle(train_df)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "test_df = pd.concat([negative_test, positive_test])\n",
        "test_df = shuffle(test_df)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "execution_count": 643,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Ci9h-DeuSbQj",
        "outputId": "6c694094-8323-45c9-b941-830fb12b3140"
      },
      "source": [
        "train_df.head(5)"
      ],
      "execution_count": 644,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>token_id</th>\n",
              "      <th>masks</th>\n",
              "      <th>segments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>LOLOLOL yeah i was</td>\n",
              "      <td>lo ##lo ##lo ##l yeah i was</td>\n",
              "      <td>[101, 8840, 4135, 4135, 2140, 3398, 1045, 2001...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Im soo boredgrrrr it sucks I want go Australia...</td>\n",
              "      <td>im soo bored ##gr ##rr ##r it sucks i want go ...</td>\n",
              "      <td>[101, 10047, 17111, 11471, 16523, 12171, 2099,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>is he that gorgeous somebody please save me</td>\n",
              "      <td>is he that gorgeous somebody please save me</td>\n",
              "      <td>[101, 2003, 2002, 2008, 9882, 8307, 3531, 3828...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Thanks You are awesome too</td>\n",
              "      <td>thanks you are awesome too</td>\n",
              "      <td>[101, 4283, 2017, 2024, 12476, 2205, 102, 0, 0...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>O how I Luvv my morning convos with  best way ...</td>\n",
              "      <td>o how i lu ##v ##v my morning con ##vos with b...</td>\n",
              "      <td>[101, 1051, 2129, 1045, 11320, 2615, 2615, 202...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion  ...                                           segments\n",
              "0        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "1        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...\n",
              "3        1  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "4        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 644
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xQXhEjk9Sdmd",
        "outputId": "57c28eff-ab2c-464f-fa82-13cc2d3221ab"
      },
      "source": [
        "test_df.head(5)"
      ],
      "execution_count": 645,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_token</th>\n",
              "      <th>token_id</th>\n",
              "      <th>masks</th>\n",
              "      <th>segments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Heading out to Staples to get my Admission Tic...</td>\n",
              "      <td>heading out to staples to get my admission tic...</td>\n",
              "      <td>[101, 5825, 2041, 2000, 24533, 2000, 2131, 202...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>really tired going to go to bed now</td>\n",
              "      <td>really tired going to go to bed now</td>\n",
              "      <td>[101, 2428, 5458, 2183, 2000, 2175, 2000, 2793...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>See No retweetsnot many followers  and I run a...</td>\n",
              "      <td>see no re ##t ##wee ##ts ##not many followers ...</td>\n",
              "      <td>[101, 2156, 2053, 2128, 2102, 28394, 3215, 170...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>has just had her hair done  just want some ext...</td>\n",
              "      <td>has just had her hair done just want some exte...</td>\n",
              "      <td>[101, 2038, 2074, 2018, 2014, 2606, 2589, 2074...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>My face is feeling it too I look red  You have...</td>\n",
              "      <td>my face is feeling it too i look red you have ...</td>\n",
              "      <td>[101, 2026, 2227, 2003, 3110, 2009, 2205, 1045...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion  ...                                           segments\n",
              "0        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1        0  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...\n",
              "2        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...\n",
              "4        1  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 645
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLS-W5iATNQs"
      },
      "source": [
        "Nice! Everything looks in order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DThBaLUpS3yn"
      },
      "source": [
        "**Lets create the feature matrix from the dataframes we've created**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCgBHWRhTj9V"
      },
      "source": [
        "Create the feature matrix(input) for train and the expected output (0 or 1 ie. negative/positive sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMkbLvvkS3oK"
      },
      "source": [
        "idx_train = [i for i in train_df[\"token_id\"]]\n",
        "masks_train = [i for i in train_df[\"masks\"]]\n",
        "segments_train = [i for i in train_df[\"segments\"]]"
      ],
      "execution_count": 646,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTNx_YZ2T5Jb"
      },
      "source": [
        "## feature matrix\n",
        "X_train = []\n",
        "X_train = [np.asarray(idx_train, dtype='int32'), \n",
        "           np.asarray(masks_train, dtype='int32'), \n",
        "           np.asarray(segments_train, dtype='int32')]"
      ],
      "execution_count": 647,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ9s508pT-h3"
      },
      "source": [
        "y_train = train_df[\"emotion\"].values"
      ],
      "execution_count": 648,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzetK4kTmxo"
      },
      "source": [
        "Create the feature matrix for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHVUaCltUXMy"
      },
      "source": [
        "idx_test = [i for i in test_df[\"token_id\"]]\n",
        "masks_test = [i for i in test_df[\"masks\"]]\n",
        "segments_test = [i for i in test_df[\"segments\"]]"
      ],
      "execution_count": 649,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCqZUi5iUXMz"
      },
      "source": [
        "## feature matrix\n",
        "X_test = []\n",
        "X_test = [np.asarray(idx_test, dtype='int32'), \n",
        "           np.asarray(masks_test, dtype='int32'), \n",
        "           np.asarray(segments_test, dtype='int32')]"
      ],
      "execution_count": 650,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv-Xh4gUUXMz"
      },
      "source": [
        "y_test = test_df[\"emotion\"].values"
      ],
      "execution_count": 651,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3rcs7DFVL1E"
      },
      "source": [
        "# **We now create the neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yitwhT6NWTuW"
      },
      "source": [
        "**Create the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILhVPDEVagg"
      },
      "source": [
        "We'll be making use of a pretrained model from bert since its available. We'll be using only 2 layers of input, so X_train and X_test must be subscript to be X_train[0:2] and X_test[0:2]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYD4gyNqVTgE",
        "outputId": "782ba3c4-cee6-4988-ff76-016d4bc26674"
      },
      "source": [
        "## inputs\n",
        "idx = layers.Input((70), dtype=\"int32\", name=\"input_idx\")        # shape is 70, cos thats what we set maxelength (pad) to be in the earlier section\n",
        "masks = layers.Input((70), dtype=\"int32\", name=\"input_masks\")\n",
        "\n",
        "\n",
        "## pre-trained bert with config (We load in a pre-trained model of BERT)\n",
        "config = transformers.DistilBertConfig(dropout=0.2, \n",
        "           attention_dropout=0.2)\n",
        "config.output_hidden_states = False\n",
        "nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n",
        "bert_out = nlp(idx, attention_mask=masks)[0]\n",
        "\n",
        "\n",
        "## fine-tuning\n",
        "x = layers.GlobalAveragePooling1D()(bert_out)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "y_out = layers.Dense(len(np.unique(y_train)), \n",
        "                     activation='softmax')(x)   # I could have used sigmoid (Meant for binary classifiation) here but it shoudnt really matter. Softmax is usally for multi-classifiaton problems.\n"
      ],
      "execution_count": 652,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9POnmyJbWYBf"
      },
      "source": [
        "**Compile and summarise the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGLuUI-LWXev",
        "outputId": "2382958f-8629-431a-8182-43d5dea2eb79"
      },
      "source": [
        "## compile\n",
        "model = models.Model([idx, masks], y_out)\n",
        "for layer in model.layers[:3]:\n",
        "    layer.trainable = False\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 653,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_idx (InputLayer)          [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 70)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_idx[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 768)          0           tf_distil_bert_model_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           49216       global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 66,412,226\n",
            "Trainable params: 49,346\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjdWNZoGXheT"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoMU4H7tZO6C"
      },
      "source": [
        "y_dict = {0: \"Negative\", 1:\"Positive\"}"
      ],
      "execution_count": 654,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogafdzQ6XDSv",
        "outputId": "455459f5-edad-4ca4-fb86-f8f2b9066a86"
      },
      "source": [
        "## train\n",
        "training = model.fit(x=X_train[0:2], y=y_train, batch_size=64, \n",
        "                     epochs=2, shuffle=True, verbose=1, \n",
        "                     validation_split=0.3)\n",
        "## test\n",
        "predicted_prob = model.predict(X_test[0:2])\n",
        "predicted = [y_dict[np.argmax(pred)] for pred in predicted_prob]"
      ],
      "execution_count": 655,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "657/657 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.7404WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "657/657 [==============================] - 149s 218ms/step - loss: 0.5220 - accuracy: 0.7404 - val_loss: 0.4893 - val_accuracy: 0.7620\n",
            "Epoch 2/2\n",
            "657/657 [==============================] - 141s 215ms/step - loss: 0.4984 - accuracy: 0.7549 - val_loss: 0.4905 - val_accuracy: 0.7621\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "APyl4l5VO8Bh",
        "outputId": "12e2e724-da59-4ced-ba0f-829638122d35"
      },
      "source": [
        "model.save('training_finalast_model.h5')"
      ],
      "execution_count": 656,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-656-a3046ad380e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_final_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2112\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m   def save_weights(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    145\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    146\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 147\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    148\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSharedObjectSavingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mmodel_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   metadata = dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_metadata\u001b[0;34m(model, include_optimizer, require_config)\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequire_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   1353\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m       \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inbound_nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    506\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    507\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 508\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2346\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2348\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JECqXcrfu_XO"
      },
      "source": [
        "X_train[0:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhN8uxOXPoFw"
      },
      "source": [
        "predicted_prob = model.predict(X_test[0:2])\n",
        "predicted = [np.argmax(pred) for pred in predicted_prob]"
      ],
      "execution_count": 657,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "N0l1JQVK9XQ-",
        "outputId": "07c50bb2-66e3-4e41-bc55-7683e04181f6"
      },
      "source": [
        "# Import the required metric from sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sb\n",
        "\n",
        "# Plot the two-way Confusion Matrix\n",
        "sb.heatmap(confusion_matrix(y_test, predicted), annot = True, fmt=\".0f\", annot_kws={\"size\": 18})"
      ],
      "execution_count": 658,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe93d26d4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 658
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFaBgNoD9sII"
      },
      "source": [
        "The model performs reasonably well, but had a greater tendency to predict tweets as negative when they're actaully positive compared to cases where tweets were predicted as positive when they were actually negative.\n",
        "\n",
        "But overall, the TN and TP rates were still high\n",
        "\n",
        "\n",
        "X = predicted\n",
        "\n",
        "Y = Actual\n",
        "\n",
        "0 = negative\n",
        "\n",
        "1 = positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATAaj8LgccdE"
      },
      "source": [
        "**You can use this to do some sanity checks based on the test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCoc5uAebdSc",
        "outputId": "663f5e5e-ea51-44b1-90e1-7d731a5a8852"
      },
      "source": [
        "k  = 25\n",
        "for i in range(k):\n",
        "  print(test_df[\"tweet\"][i], predicted[i], y_train[i])\n"
      ],
      "execution_count": 659,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heading out to Staples to get my Admission Ticket out for the SATs tomorrow THEN get my food  0 0\n",
            "really tired going to go to bed now  0 1\n",
            "See No retweetsnot many followers  and I run and Ive hand copied the new testament too Interesting  0 0\n",
            "has just had her hair done  just want some extensions 0 1\n",
            "My face is feeling it too I look red  You have to admit it was fun 0 1\n",
            "Getting ready to give Em her first driving lesson Shes going to do great If you can drive this van you can drive anything  1 0\n",
            "About to make popcorn and cuddle up and watch the movie awards  1 1\n",
            "theres nothing on tv and school tomorrow  SAVE MEE 0 0\n",
            "watching the Dark Knight  1 1\n",
            "says back in Bacolod  downloading pictures 1 0\n",
            "bradie your a real man  1 0\n",
            "that hurt my ears  hehe 0 0\n",
            "We bought Dr Horrible on iTunes Its very good worth the price IMO But would be nice to watch it on Hulu  0 1\n",
            "Missing Sexy Security Council  0 1\n",
            "is going to see Thriller tonight  1 0\n",
            "I miss my love sooooo much    0 0\n",
            "One week  till Three days 0 0\n",
            "Cant believe today marks the year anniversary that Another World has been off the air  I truly miss it 0 0\n",
            "getting ready to go to a funeral today not fun  0 1\n",
            "Coffee is almost gone  0 1\n",
            "hi rod  still booming 1 1\n",
            "Only ten days till I can see Oh Im gona fall in love all over again  still wkn then going to watch some enterage 0 1\n",
            "you are welcome to drive by and wave if youd like  1 1\n",
            "I NEED TO GET BROADBAND AT MY DADS Also does anyone have any USB modems lying around that they would be willing to give me  0 0\n",
            "Thank you for the  1 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxW1sCDJgIQR"
      },
      "source": [
        "**Remember, 0 = Negative, and 1 = Positive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uAtgpPgd74C"
      },
      "source": [
        "## INTERMISIONNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "554tI2SFbiS_"
      },
      "source": [
        "df = pd.read_csv(\"gdrive/My Drive/twitter_data.csv\", encoding='latin-1', header= None) # Read the data in\n",
        "df = df.drop(columns= [0, 1, 2, 3, 4])  # Drop all useless data\n",
        "df.columns = ['tweet'] # Rename the relavant columns\n",
        "\n",
        "# create new df containing negative sentiment\n",
        "df_pred = df[69420:69421]\n",
        "\n",
        "df_pred = df_pred.reset_index(drop=True)"
      ],
      "execution_count": 702,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVCPTgxalyNt"
      },
      "source": [
        "Ask for input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJSEA9O94Nd9"
      },
      "source": [
        "def check_the_tweet(user_input):\n",
        "  df_pred[\"tweet\"][0] = user_input\n",
        "\n",
        "  # these regexesr enable us to filter out unwanted data\n",
        "  regex1 = re.compile('@') # format to remove @username_chungus from a string\n",
        "  regex2 = re.compile('#') # format to remove #Ole'sAtTheWheel from a string\n",
        "  regex3 = re.compile('http') # format to remove http/https://blahblah.com (Dont blame me if you click this link i dunno where it goes) from a string\n",
        "  regex4 = re.compile('www') # format to remove www.blahblah.com from a string\n",
        "  regex5 = re.compile('\\d') # format to remove numbers from a string\n",
        "  regex6 = re.compile(r'[^\\w\\s]') # format to remove punctuations from a list\n",
        "\n",
        "\n",
        "  process_pred = df_pred['tweet'][0]\n",
        "  process_pred = process_pred.split(\" \")\n",
        "    \n",
        "  process_pred = [s for s in process_pred if not regex1.match(s)]\n",
        "  process_pred = [s for s in process_pred if not regex2.match(s)]\n",
        "  process_pred = [s for s in process_pred if not regex3.match(s)]\n",
        "  process_pred = [s for s in process_pred if not regex4.match(s)]\n",
        "  process_pred = [s for s in process_pred if not regex5.match(s)]\n",
        "  process_pred = [s for s in process_pred if not regex6.match(s)]\n",
        "\n",
        "    \n",
        "\n",
        "  df_pred['tweet'][0] =  \" \".join(process_pred)\n",
        "  df_pred['tweet'][0] = re.sub(r'[^\\w\\s]', '', df_pred['tweet'][0]) # format to remove punctuations from a string\n",
        "\n",
        "  df_pred['tweet_token'] = 'hi'\n",
        "\n",
        "  df_pred['tweet_token'][0] = \" \".join(  tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', df_pred['tweet'][0].lower().strip()))  )\n",
        "\n",
        "  maxlen = 70\n",
        "\n",
        "  ## add special tokens\n",
        "  maxqnans = np.int(maxlen-20)  # The tokenized from would end up being too long as seen by the values of mn and np, thus we would just take the first #maxqns (70 - 20 = 50) number of tokens \n",
        "  df_pred_tokenized_list = [\"[CLS] \"+  \" \".join(txt.split(\" \")[:maxqnans]) + \" [SEP]\" for txt in df_pred[\"tweet_token\"]]\n",
        "\n",
        "  ## generate masks for pred\n",
        "  masks_pred = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(txt.split(\" \"))) for txt in df_pred_tokenized_list]\n",
        "\n",
        "  ## padding for pred\n",
        "  txt2seq_pred = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in df_pred_tokenized_list]\n",
        "\n",
        "  ## generate idx for pred\n",
        "  idx_pred = []\n",
        "\n",
        "  for seq in txt2seq_pred:\n",
        "    the_id = tokenizer.convert_tokens_to_ids(seq.split(\" \")) \n",
        "    idx_pred.append(the_id)\n",
        "\n",
        "  ## generate segments for pred\n",
        "  segments_pred = [] \n",
        "  for seq in txt2seq_pred:\n",
        "    temp, i = [], 0\n",
        "    for token in seq.split(\" \"):\n",
        "      if token != \"[SEP]\":\n",
        "        temp.append(i)\n",
        "      else:\n",
        "        i = 1\n",
        "        temp.append(i)\n",
        "    segments_pred.append(temp)\n",
        "\n",
        "\n",
        "  df_pred[\"token_id\"] = idx_pred\n",
        "  df_pred[\"masks\"] = masks_pred\n",
        "  df_pred[\"segments\"] = segments_pred\n",
        "\n",
        "  idx_pred = [i for i in df_pred[\"token_id\"]]\n",
        "  masks_pred = [i for i in df_pred[\"masks\"]]\n",
        "\n",
        "  ## feature matrix\n",
        "  X_pred = []\n",
        "  X_pred = [np.asarray(idx_pred, dtype='int32'), \n",
        "            np.asarray(masks_pred, dtype='int32')]\n",
        "\n",
        "  ## make the prediction\n",
        "  y_pred = pred_df[\"emotion\"].values\n",
        "\n",
        "  ## get the predictio array\n",
        "  pred_prob = model.predict(X_pred)\n",
        "  pred_list = [np.argmax(pred) for pred in pred_prob]\n",
        "\n",
        "  dict_sentiment = {0: \"Negative\", 1:\"Positive\"}\n",
        "\n",
        "  k = 0\n",
        "  print(df_pred[\"tweet\"][k])\n",
        "  print(\"The predicton is\", dict_sentiment[pred_list[k]])"
      ],
      "execution_count": 706,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJf5GuXe5IEA",
        "outputId": "219a9e8d-4c53-4868-a7d4-66918fad3cab"
      },
      "source": [
        "check_the_tweet( input(\"Enter the tweet you want to analyse: \") )"
      ],
      "execution_count": 708,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the tweet you want to analyse: rooney id he GOAT hes amazing. a wizard\n",
            "rooney id he GOAT hes amazing a wizard\n",
            "The predicton is Positive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}